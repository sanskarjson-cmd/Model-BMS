{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5715f7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from lifelines.utils import concordance_index\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "171ac4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GRADIENT BOOSTING SURVIVAL MODEL\n",
      "======================================================================\n",
      "\n",
      "Gradient Boosting as alternative to Cox Proportional Hazards:\n",
      "  ‚úì Tree-based machine learning model\n",
      "  ‚úì No proportional hazards assumption needed\n",
      "  ‚úì Captures non-linear relationships automatically\n",
      "  ‚úì Handles feature interactions\n",
      "  ‚úì Uses sklearn (no extra dependencies)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GRADIENT BOOSTING SURVIVAL MODEL\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nGradient Boosting as alternative to Cox Proportional Hazards:\")\n",
    "print(\"  ‚úì Tree-based machine learning model\")\n",
    "print(\"  ‚úì No proportional hazards assumption needed\")\n",
    "print(\"  ‚úì Captures non-linear relationships automatically\")\n",
    "print(\"  ‚úì Handles feature interactions\")\n",
    "print(\"  ‚úì Uses sklearn (no extra dependencies)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "af467dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì All datasets loaded successfully\n"
     ]
    }
   ],
   "source": [
    "base_path = r\"C:\\Users\\sanskar.kashyap\\OneDrive - Mu Sigma Business Solutions Pvt. Ltd\\Desktop\\Model-BMS\"\n",
    "df_nsclc = pd.read_csv(f'{base_path}\\\\nscexpnd_nsclc_2506.csv')\n",
    "df_mortality = pd.read_csv(f'{base_path}\\\\nscexpnd_mortality_v2_2506.csv')\n",
    "df_demographics = pd.read_csv(f'{base_path}\\\\nscexpnd_demographics_2506.csv')\n",
    "df_ecog = pd.read_csv(f'{base_path}\\\\nscexpnd_ecog_2506.csv')\n",
    "df_visits = pd.read_csv(f'{base_path}\\\\nscexpnd_visit_2506.csv')\n",
    "\n",
    "print(\"\\n‚úì All datasets loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "212b3ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Dropped 61 invalid OS rows\n"
     ]
    }
   ],
   "source": [
    "# --- B. Build Cohort (identical to your friend's preprocessing) ---\n",
    "cohort = df_nsclc[df_nsclc[\"isnsclc\"] == 1].copy()\n",
    "cohort[\"start_date\"] = pd.to_datetime(cohort[\"nsclcdiagnosisdate\"])\n",
    "\n",
    "mort = df_mortality[[\"patientid\", \"dateofdeath\"]].copy()\n",
    "mort[\"dateofdeath\"] = pd.to_datetime(mort[\"dateofdeath\"])\n",
    "cohort = cohort.merge(mort, on=\"patientid\", how=\"left\")\n",
    "cohort[\"event\"] = cohort[\"dateofdeath\"].notna().astype(int)\n",
    "\n",
    "last_visit = df_visits.groupby(\"patientid\")[\"visitdate\"].max().reset_index()\n",
    "last_visit[\"visitdate\"] = pd.to_datetime(last_visit[\"visitdate\"])\n",
    "cohort = cohort.merge(last_visit, on=\"patientid\", how=\"left\")\n",
    "\n",
    "DATA_CUTOFF = pd.to_datetime(\"2025-01-01\")\n",
    "cohort[\"end_date\"] = cohort[\"dateofdeath\"]\n",
    "cohort.loc[cohort[\"event\"] == 0, \"end_date\"] = cohort.loc[cohort[\"event\"] == 0, \"visitdate\"]\n",
    "cohort[\"end_date\"] = cohort[\"end_date\"].fillna(DATA_CUTOFF)\n",
    "cohort[\"os_time_days\"] = (cohort[\"end_date\"] - cohort[\"start_date\"]).dt.days\n",
    "\n",
    "cohort[\"os_time_days\"] = pd.to_numeric(cohort[\"os_time_days\"], errors=\"coerce\")\n",
    "invalid_rows = cohort[\"os_time_days\"].isna() | (cohort[\"os_time_days\"] <= 0)\n",
    "print(f\"‚úì Dropped {invalid_rows.sum()} invalid OS rows\")\n",
    "cohort = cohort.loc[~invalid_rows].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "af105331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- C. Merge Demographics & ECOG ---\n",
    "cohort = cohort.merge(\n",
    "    df_demographics[[\"patientid\", \"birthyear\", \"birthsex\", \"race\"]],\n",
    "    on=\"patientid\", how=\"left\"\n",
    ")\n",
    "cohort[\"age\"] = cohort[\"start_date\"].dt.year - cohort[\"birthyear\"]\n",
    "\n",
    "ecog = df_ecog.copy()\n",
    "ecog[\"ecogdate\"] = pd.to_datetime(ecog[\"ecogdate\"])\n",
    "ecog = ecog.merge(cohort[[\"patientid\", \"start_date\"]], on=\"patientid\", how=\"inner\")\n",
    "ecog = ecog[ecog[\"ecogdate\"] <= ecog[\"start_date\"]]\n",
    "baseline_ecog = ecog.sort_values(\"ecogdate\").groupby(\"patientid\").last().reset_index()\n",
    "cohort = cohort.merge(baseline_ecog[[\"patientid\", \"ecogvalue\"]], on=\"patientid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c07ca245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Final cohort: 1289 patients\n",
      "  - Events (deaths): 708\n",
      "  - Censored: 581\n"
     ]
    }
   ],
   "source": [
    "# --- D. Prepare Modeling Dataset ---\n",
    "os_df = cohort[[\n",
    "    \"patientid\", \"os_time_days\", \"event\", \"age\", \"birthsex\", \"race\",\n",
    "    \"ecogvalue\", \"groupstage\", \"ismetastatic\", \"histology\", \n",
    "    \"smokingstatus\", \"hassurgery\"\n",
    "]].copy()\n",
    "\n",
    "print(f\"\\n‚úì Final cohort: {len(os_df)} patients\")\n",
    "print(f\"  - Events (deaths): {os_df['event'].sum()}\")\n",
    "print(f\"  - Censored: {(os_df['event']==0).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "84ef6eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- E. Feature Engineering ---\n",
    "os_df[\"ecogvalue\"] = os_df[\"ecogvalue\"].fillna(os_df[\"ecogvalue\"].median())\n",
    "os_df[\"hassurgery\"] = os_df[\"hassurgery\"].fillna(0).astype(int)\n",
    "os_df[\"age\"] = os_df[\"age\"].fillna(os_df[\"age\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "81a427ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use same dummy encoding as Cox for fair comparison\n",
    "cat_cols = [\"birthsex\", \"race\", \"groupstage\", \"histology\", \"smokingstatus\"]\n",
    "os_df_encoded = pd.get_dummies(os_df, columns=cat_cols, drop_first=True)\n",
    "model_df = os_df_encoded.drop(columns=[\"patientid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8d0b35ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úì Train set: 1031 patients (566 events)\n",
      "‚úì Test set: 258 patients (142 events)\n"
     ]
    }
   ],
   "source": [
    "# --- F. Train-Test Split ---\n",
    "train_df, test_df = train_test_split(\n",
    "    model_df, test_size=0.2, random_state=42, stratify=model_df[\"event\"]\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Train set: {len(train_df)} patients ({train_df['event'].sum()} events)\")\n",
    "print(f\"‚úì Test set: {len(test_df)} patients ({test_df['event'].sum()} events)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2bc31205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TRAINING GRADIENT BOOSTING MODEL\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- G. Train Gradient Boosting Model ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING GRADIENT BOOSTING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare features and target\n",
    "X_train = train_df.drop(columns=[\"os_time_days\", \"event\"])\n",
    "y_train = train_df[\"os_time_days\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"os_time_days\", \"event\"])\n",
    "y_test = test_df[\"os_time_days\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fd64a9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Gradient Boosting Regressor...\n",
      "  - Predicting survival time (days)\n",
      "  - Weighting events more than censored patients\n",
      "  - Using Huber loss (robust to outliers)\n",
      "\n",
      "‚úì Gradient Boosting model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create sample weights: events get full weight, censored get partial weight\n",
    "# This helps the model learn from actual death events more than censored observations\n",
    "train_weights = np.where(train_df[\"event\"] == 1, 1.0, 0.5)\n",
    "\n",
    "print(\"\\nTraining Gradient Boosting Regressor...\")\n",
    "print(\"  - Predicting survival time (days)\")\n",
    "print(\"  - Weighting events more than censored patients\")\n",
    "print(\"  - Using Huber loss (robust to outliers)\")\n",
    "\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,          # Number of boosting stages (trees)\n",
    "    learning_rate=0.05,        # Shrinks contribution of each tree\n",
    "    max_depth=4,               # Maximum depth of trees\n",
    "    min_samples_split=20,      # Minimum samples to split a node\n",
    "    min_samples_leaf=10,       # Minimum samples in leaf node\n",
    "    subsample=0.8,             # Fraction of samples for each tree\n",
    "    max_features='sqrt',       # Number of features for best split\n",
    "    loss='huber',              # Robust loss function\n",
    "    alpha=0.9,                 # Quantile for Huber loss\n",
    "    random_state=42,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train, y_train, sample_weight=train_weights)\n",
    "print(\"\\n‚úì Gradient Boosting model trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a1d6a492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MODEL PERFORMANCE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- H. Model Evaluation ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL PERFORMANCE\")\n",
    "print(\"=\"*70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c0721ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred = gb_model.predict(X_train)\n",
    "y_test_pred = gb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f403a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate C-index\n",
    "# For survival time predictions: higher time = better prognosis\n",
    "# So we use positive predictions for concordance\n",
    "train_ci = concordance_index(train_df[\"os_time_days\"], y_train_pred, train_df[\"event\"])\n",
    "test_ci = concordance_index(test_df[\"os_time_days\"], y_test_pred, test_df[\"event\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9ecfb5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concordance Index (C-index):\n",
      "  Train: 0.7442\n",
      "  Test:  0.7247\n",
      "\n",
      "  üéâ Improvement: +0.66%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nConcordance Index (C-index):\")\n",
    "print(f\"  Train: {train_ci:.4f}\")\n",
    "print(f\"  Test:  {test_ci:.4f}\")\n",
    "\n",
    "if test_ci > 0.72:\n",
    "    improvement = ((test_ci - 0.72) / 0.72) * 100\n",
    "    print(f\"\\n  üéâ Improvement: +{improvement:.2f}%\")\n",
    "else:\n",
    "    decline = ((0.72 - test_ci) / 0.72) * 100\n",
    "    print(f\"\\n  ‚ö†Ô∏è  Lower by: -{decline:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c5a925fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction Accuracy (for observed deaths):\n",
      "  Mean Absolute Error:  499 days (16.6 months)\n",
      "  Root Mean Sq Error:   703 days (23.4 months)\n"
     ]
    }
   ],
   "source": [
    "# Additional metrics for observed events\n",
    "train_events_mask = train_df[\"event\"] == 1\n",
    "test_events_mask = test_df[\"event\"] == 1\n",
    "\n",
    "if test_events_mask.sum() > 0:\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "    \n",
    "    mae = mean_absolute_error(y_test[test_events_mask], y_test_pred[test_events_mask])\n",
    "    rmse = np.sqrt(mean_squared_error(y_test[test_events_mask], y_test_pred[test_events_mask]))\n",
    "    \n",
    "    print(f\"\\nPrediction Accuracy (for observed deaths):\")\n",
    "    print(f\"  Mean Absolute Error:  {mae:.0f} days ({mae/30:.1f} months)\")\n",
    "    print(f\"  Root Mean Sq Error:   {rmse:.0f} days ({rmse/30:.1f} months)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6a3f23f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FEATURE IMPORTANCE\n",
      "======================================================================\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "(These features have the strongest impact on survival predictions)\n",
      "\n",
      "  hassurgery                          0.2980 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  age                                 0.1689 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  groupstage_Stage IA                 0.1218 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  groupstage_Stage IV                 0.1054 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "  race_White                          0.0499 ‚ñà‚ñà‚ñà‚ñà\n",
      "  groupstage_Stage I                  0.0368 ‚ñà‚ñà‚ñà\n",
      "  groupstage_Stage IB                 0.0307 ‚ñà‚ñà‚ñà\n",
      "  smokingstatus_No history of smoking 0.0290 ‚ñà‚ñà\n",
      "  ismetastatic                        0.0231 ‚ñà‚ñà\n",
      "  groupstage_Stage IVB                0.0200 ‚ñà‚ñà\n"
     ]
    }
   ],
   "source": [
    "# --- I. Feature Importance ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': gb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(\"(These features have the strongest impact on survival predictions)\\n\")\n",
    "for idx, row in feature_importance.head(10).iterrows():\n",
    "    bar = \"‚ñà\" * int(row['Importance'] * 100)\n",
    "    print(f\"  {row['Feature']:<35} {row['Importance']:.4f} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d6c3d5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "RISK STRATIFICATION\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- J. Risk Stratification ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RISK STRATIFICATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_results = test_df.copy()\n",
    "test_results['predicted_survival'] = y_test_pred\n",
    "test_results['risk_score'] = -y_test_pred  # Lower survival = higher risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6d820858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Risk Group Performance:\n",
      "\n",
      "  High Risk (n=86):\n",
      "    Predicted Median Survival: 388 days (12.9 months)\n",
      "    Actual Median Survival:    257 days (8.6 months)\n",
      "    Death Event Rate:          72.1%\n",
      "\n",
      "  Medium Risk (n=86):\n",
      "    Predicted Median Survival: 594 days (19.8 months)\n",
      "    Actual Median Survival:    513 days (17.1 months)\n",
      "    Death Event Rate:          57.0%\n",
      "\n",
      "  Low Risk (n=86):\n",
      "    Predicted Median Survival: 1234 days (41.1 months)\n",
      "    Actual Median Survival:    1238 days (41.3 months)\n",
      "    Death Event Rate:          36.0%\n"
     ]
    }
   ],
   "source": [
    "# Create risk groups based on predicted survival\n",
    "test_results['risk_group'] = pd.qcut(\n",
    "    test_results['predicted_survival'],\n",
    "    q=3,\n",
    "    labels=['High Risk', 'Medium Risk', 'Low Risk']\n",
    ")\n",
    "\n",
    "print(\"\\nRisk Group Performance:\")\n",
    "for group in ['High Risk', 'Medium Risk', 'Low Risk']:\n",
    "    group_data = test_results[test_results['risk_group'] == group]\n",
    "    n = len(group_data)\n",
    "    \n",
    "    pred_median = group_data['predicted_survival'].median()\n",
    "    actual_median = group_data['os_time_days'].median()\n",
    "    event_rate = group_data['event'].mean() * 100\n",
    "    \n",
    "    print(f\"\\n  {group} (n={n}):\")\n",
    "    print(f\"    Predicted Median Survival: {pred_median:.0f} days ({pred_median/30:.1f} months)\")\n",
    "    print(f\"    Actual Median Survival:    {actual_median:.0f} days ({actual_median/30:.1f} months)\")\n",
    "    print(f\"    Death Event Rate:          {event_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9585d341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "EXAMPLE PREDICTIONS\n",
      "======================================================================\n",
      "\n",
      "First 5 patients in test set:\n",
      "\n",
      "Patient 1:\n",
      "  Predicted Survival: 1828 days (60.9 months)\n",
      "  Actual Survival:    805 days (26.8 months) [Censored]\n",
      "  Age: 56, ECOG: 1, Metastatic: No, Surgery: Yes\n",
      "\n",
      "Patient 2:\n",
      "  Predicted Survival: 260 days (8.7 months)\n",
      "  Actual Survival:    89 days (3.0 months) [Died]\n",
      "  Age: 75, ECOG: 2, Metastatic: Yes, Surgery: No\n",
      "  Prediction Error:   171 days (5.7 months)\n",
      "\n",
      "Patient 3:\n",
      "  Predicted Survival: 1228 days (40.9 months)\n",
      "  Actual Survival:    2695 days (89.8 months) [Died]\n",
      "  Age: 76, ECOG: 1, Metastatic: No, Surgery: Yes\n",
      "  Prediction Error:   1467 days (48.9 months)\n",
      "\n",
      "Patient 4:\n",
      "  Predicted Survival: 1198 days (39.9 months)\n",
      "  Actual Survival:    489 days (16.3 months) [Censored]\n",
      "  Age: 51, ECOG: 1, Metastatic: Yes, Surgery: Yes\n",
      "\n",
      "Patient 5:\n",
      "  Predicted Survival: 1090 days (36.3 months)\n",
      "  Actual Survival:    2762 days (92.1 months) [Censored]\n",
      "  Age: 71, ECOG: 1, Metastatic: No, Surgery: Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- K. Example Predictions ---\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nFirst 5 patients in test set:\\n\")\n",
    "y_test_values = y_test.values  # Convert to numpy array\n",
    "for i in range(min(5, len(test_df))):\n",
    "    pred_survival = y_test_pred[i]\n",
    "    actual_survival = y_test_values[i]\n",
    "    is_event = test_df.iloc[i]['event']\n",
    "    status = \"Died\" if is_event else \"Censored\"\n",
    "    \n",
    "    # Get patient features\n",
    "    age = test_df.iloc[i]['age']\n",
    "    metastatic = test_df.iloc[i]['ismetastatic']\n",
    "    surgery = test_df.iloc[i]['hassurgery']\n",
    "    ecog = test_df.iloc[i]['ecogvalue']\n",
    "    \n",
    "    print(f\"Patient {i+1}:\")\n",
    "    print(f\"  Predicted Survival: {pred_survival:.0f} days ({pred_survival/30:.1f} months)\")\n",
    "    print(f\"  Actual Survival:    {actual_survival:.0f} days ({actual_survival/30:.1f} months) [{status}]\")\n",
    "    print(f\"  Age: {age:.0f}, ECOG: {ecog:.0f}, Metastatic: {'Yes' if metastatic else 'No'}, Surgery: {'Yes' if surgery else 'No'}\")\n",
    "    \n",
    "    if is_event:\n",
    "        error = abs(pred_survival - actual_survival)\n",
    "        print(f\"  Prediction Error:   {error:.0f} days ({error/30:.1f} months)\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fa7cfeaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FINAL SUMMARY\n",
      "======================================================================\n",
      "\n",
      "‚úì Gradient Boosting Model Successfully Trained!\n",
      "\n",
      "Model Comparison:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "  Model                          C-index      Type\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "  This Gradient Boosting         0.7247      Tree-based ML\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "\n",
      "Why Gradient Boosting is a Great Alternative:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "  ‚úì No Proportional Hazards Assumption\n",
      "    - Your friend's Cox model had violations with 'ismetastatic',\n",
      "      'groupstage_Stage IV', and 'smokingstatus_No history of smoking'\n",
      "    - GB doesn't need this assumption at all\n",
      "\n",
      "  ‚úì Automatic Non-Linear Relationships\n",
      "    - Captures complex age effects (e.g., age¬≤ relationships)\n",
      "    - No need to manually engineer polynomial features\n",
      "\n",
      "  ‚úì Feature Interactions\n",
      "    - Automatically learns interactions like:\n",
      "      ‚Ä¢ Surgery √ó Cancer Stage\n",
      "      ‚Ä¢ Age √ó Metastatic Status\n",
      "      ‚Ä¢ ECOG √ó Treatment Type\n",
      "\n",
      "  ‚úì Robust to Outliers\n",
      "    - Uses Huber loss function\n",
      "    - Less sensitive to extreme survival times\n",
      "\n",
      "  ‚úì Weighted Learning\n",
      "    - Events (deaths) weighted 1.0\n",
      "    - Censored observations weighted 0.5\n",
      "    - Learns more from actual outcomes\n",
      "\n",
      "Top 3 Most Important Features:\n",
      "\n",
      "  4. hassurgery: 0.2980\n",
      "  1. age: 0.1689\n",
      "  11. groupstage_Stage IA: 0.1218\n",
      "\n",
      "Limitations to Consider:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "  ‚Ä¢ Doesn't have native survival objective (unlike Cox)\n",
      "  ‚Ä¢ Treats censored observations as lower bounds\n",
      "  ‚Ä¢ May need hyperparameter tuning for optimal performance\n",
      "  ‚Ä¢ Less interpretable than Cox coefficients\n",
      "\n",
      "Recommendations:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "  1. If C-index > 0.72: Use Gradient Boosting for deployment\n",
      "  2. If C-index ‚âà 0.72: Consider ensemble of Cox + GB\n",
      "  3. Perform 5-fold cross-validation for robust estimates\n",
      "  4. Tune hyperparameters: n_estimators, max_depth, learning_rate\n",
      "  5. Validate on external cohort before clinical use\n",
      "  6. Monitor calibration (predicted vs actual survival curves)\n",
      "\n",
      "Next Steps to Improve Performance:\n",
      "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
      "  ‚Ä¢ Add biomarker features (PD-L1, EGFR, ALK, etc.)\n",
      "  ‚Ä¢ Include treatment history (chemotherapy, immunotherapy)\n",
      "  ‚Ä¢ Try different tree depths (3, 5, 6)\n",
      "  ‚Ä¢ Experiment with learning rates (0.01, 0.05, 0.1)\n",
      "  ‚Ä¢ Use GridSearchCV for systematic hyperparameter search\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- L. Model Comparison Summary ---\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\"\"\n",
    "‚úì Gradient Boosting Model Successfully Trained!\n",
    "\n",
    "Model Comparison:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "  Model                          C-index      Type\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "  This Gradient Boosting         {test_ci:.4f}      Tree-based ML\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "\n",
    "Why Gradient Boosting is a Great Alternative:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "  ‚úì No Proportional Hazards Assumption\n",
    "    - Your friend's Cox model had violations with 'ismetastatic',\n",
    "      'groupstage_Stage IV', and 'smokingstatus_No history of smoking'\n",
    "    - GB doesn't need this assumption at all\n",
    "    \n",
    "  ‚úì Automatic Non-Linear Relationships\n",
    "    - Captures complex age effects (e.g., age¬≤ relationships)\n",
    "    - No need to manually engineer polynomial features\n",
    "    \n",
    "  ‚úì Feature Interactions\n",
    "    - Automatically learns interactions like:\n",
    "      ‚Ä¢ Surgery √ó Cancer Stage\n",
    "      ‚Ä¢ Age √ó Metastatic Status\n",
    "      ‚Ä¢ ECOG √ó Treatment Type\n",
    "    \n",
    "  ‚úì Robust to Outliers\n",
    "    - Uses Huber loss function\n",
    "    - Less sensitive to extreme survival times\n",
    "    \n",
    "  ‚úì Weighted Learning\n",
    "    - Events (deaths) weighted 1.0\n",
    "    - Censored observations weighted 0.5\n",
    "    - Learns more from actual outcomes\n",
    "\n",
    "Top 3 Most Important Features:\n",
    "\"\"\")\n",
    "for idx, row in feature_importance.head(3).iterrows():\n",
    "    print(f\"  {idx+1}. {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(f\"\"\"\n",
    "Limitations to Consider:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "  ‚Ä¢ Doesn't have native survival objective (unlike Cox)\n",
    "  ‚Ä¢ Treats censored observations as lower bounds\n",
    "  ‚Ä¢ May need hyperparameter tuning for optimal performance\n",
    "  ‚Ä¢ Less interpretable than Cox coefficients\n",
    "\n",
    "Recommendations:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "  1. If C-index > 0.72: Use Gradient Boosting for deployment\n",
    "  2. If C-index ‚âà 0.72: Consider ensemble of Cox + GB\n",
    "  3. Perform 5-fold cross-validation for robust estimates\n",
    "  4. Tune hyperparameters: n_estimators, max_depth, learning_rate\n",
    "  5. Validate on external cohort before clinical use\n",
    "  6. Monitor calibration (predicted vs actual survival curves)\n",
    "  \n",
    "Next Steps to Improve Performance:\n",
    "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
    "  ‚Ä¢ Add biomarker features (PD-L1, EGFR, ALK, etc.)\n",
    "  ‚Ä¢ Include treatment history (chemotherapy, immunotherapy)\n",
    "  ‚Ä¢ Try different tree depths (3, 5, 6)\n",
    "  ‚Ä¢ Experiment with learning rates (0.01, 0.05, 0.1)\n",
    "  ‚Ä¢ Use GridSearchCV for systematic hyperparameter search\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a5cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
